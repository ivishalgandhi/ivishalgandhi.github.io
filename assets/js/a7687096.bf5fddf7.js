(self.webpackChunk=self.webpackChunk||[]).push([[1013],{24508:(e,n,t)=>{var r={"./locale":25762,"./locale.js":25762};function a(e){var n=o(e);return t(n)}function o(e){if(!t.o(r,e)){var n=new Error("Cannot find module '"+e+"'");throw n.code="MODULE_NOT_FOUND",n}return r[e]}a.keys=function(){return Object.keys(r)},a.resolve=o,e.exports=a,a.id=24508},92069:(e,n,t)=>{"use strict";t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var r=t(58168),a=(t(96540),t(15680)),o=t(69837);const i={slug:"mongodb-replicaset-write-concern-read-pref",title:"Understanding MongoDB Replicasets and Write Concern - Part 1",authors:{name:"Vishal Gandhi",url:"https://github.com/ivishalgandhi",image_url:"https://github.com/ivishalgandhi.png"},tags:["mongodb","replicaset","write-concern"]},s=void 0,l={permalink:"/mongodb-replicaset-write-concern-read-pref",source:"@site/blog/2022-08-21-mongodb-replicaset-write-concern-read-pref.md",title:"Understanding MongoDB Replicasets and Write Concern - Part 1",description:"Introducing Replicasets",date:"2022-08-21T00:00:00.000Z",formattedDate:"August 21, 2022",tags:[{label:"mongodb",permalink:"/tags/mongodb"},{label:"replicaset",permalink:"/tags/replicaset"},{label:"write-concern",permalink:"/tags/write-concern"}],readingTime:6.25,hasTruncateMarker:!0,authors:[{name:"Vishal Gandhi",url:"https://github.com/ivishalgandhi",image_url:"https://github.com/ivishalgandhi.png",imageURL:"https://github.com/ivishalgandhi.png"}],frontMatter:{slug:"mongodb-replicaset-write-concern-read-pref",title:"Understanding MongoDB Replicasets and Write Concern - Part 1",authors:{name:"Vishal Gandhi",url:"https://github.com/ivishalgandhi",image_url:"https://github.com/ivishalgandhi.png",imageURL:"https://github.com/ivishalgandhi.png"},tags:["mongodb","replicaset","write-concern"]},prevItem:{title:"Understanding REST API Design Rules",permalink:"/rest-api-design-rules"},nextItem:{title:"Guide to Software Bill of Materials(SBoM) and Docker SBOM CLI",permalink:"/docker-sbom"}},c={authorsImageUrls:[void 0]},d=[{value:"Introducing Replicasets",id:"introducing-replicasets",level:2},{value:"Understanding Oplog",id:"understanding-oplog",level:2},{value:"Understanding Write Concern",id:"understanding-write-concern",level:2},{value:"Default Write Concern",id:"default-write-concern",level:3},{value:"Cluster-Wide Write Concern",id:"cluster-wide-write-concern",level:4},{value:"Implicit default write concern",id:"implicit-default-write-concern",level:4},{value:"PSA",id:"psa",level:5},{value:"Sharded Cluster",id:"sharded-cluster",level:5},{value:"Understanding Secondary Nodes Operations",id:"understanding-secondary-nodes-operations",level:2},{value:"Oplog Fetching",id:"oplog-fetching",level:3}],p={toc:d},g="wrapper";function m(e){let{components:n,...t}=e;return(0,a.yg)(g,(0,r.A)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,a.yg)("h2",{id:"introducing-replicasets"},"Introducing Replicasets"),(0,a.yg)("p",null,"The way to achieve fault tolerance in MongoDB is through the use of ",(0,a.yg)("inlineCode",{parentName:"p"},"replica sets"),". "),(0,a.yg)(o.Mermaid,{config:{},chart:"stateDiagram-v2\n    [*] --\x3e Application\n    direction LR\n    state Application\n    Application --\x3e replicaset      \n   state replicaset\n    {\n    direction RL\n    Primary:primary\n    Secondary1:secondary \n    Secondary2:secondary\n    Secondary1--\x3ePrimary : Fetch Oplog\n    Secondary2--\x3ePrimary : Fetch Oplog\n    \n    }",mdxType:"Mermaid"}),(0,a.yg)("p",null,"Two or more ",(0,a.yg)("inlineCode",{parentName:"p"},"secondary")," nodes along with a ",(0,a.yg)("inlineCode",{parentName:"p"},"primary")," node forms a replica set. Application makes all the read/write calls to the primary node which propagate all the write requests synchronously or asynchronously to the secondary nodes. "),(0,a.yg)("p",null,"The Secondary nodes fetches the data via Oplog pull from Primary or other nodes. "),(0,a.yg)("p",null,"The Primary node is responsible for all the writes and reads. The secondary nodes can be utilized for reads via ",(0,a.yg)("a",{parentName:"p",href:"https://docs.mongodb.com/manual/reference/method/Mongo.setSecondaryOk/"},(0,a.yg)("inlineCode",{parentName:"a"},"setSecondaryOk"))," or ",(0,a.yg)("a",{parentName:"p",href:"https://docs.mongodb.com/manual/reference/read-preference/"},(0,a.yg)("inlineCode",{parentName:"a"},"readPreference")),". "),(0,a.yg)("h2",{id:"understanding-oplog"},"Understanding Oplog"),(0,a.yg)("p",null,"When the application performs a write, the primary node applies the write to the database like a standalone. "),(0,a.yg)("p",null,"The difference between Replicaset write and standalone write is that replica set nodes have an ",(0,a.yg)("inlineCode",{parentName:"p"},"OpObserver")," that inserts a document to the ",(0,a.yg)("strong",{parentName:"p"},"oplog")," whenever a write to the database happens, describing the write. The ",(0,a.yg)("strong",{parentName:"p"},"oplog")," is a capped collection called ",(0,a.yg)("inlineCode",{parentName:"p"},"oplog.rs")," in the ",(0,a.yg)("inlineCode",{parentName:"p"},"local")," database. "),(0,a.yg)("p",null,"For every operation performed in a write, the primary node inserts a document into the oplog. The oplog is a capped collection, which means that it has a maximum size. When the oplog reaches its maximum size, MongoDB removes the oldest entries to make room for new entries. "),(0,a.yg)("p",null,"For a write which performs create collection and insert, there are two oplog entries created one for ",(0,a.yg)("inlineCode",{parentName:"p"},"create")," collection and another for ",(0,a.yg)("inlineCode",{parentName:"p"},"insert"),"."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-cpp"},"// mongod_main.cpp\nsetUpObservers(service);\n")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-cpp"},"\n//op_observer_registry.h\nvoid onCreateCollection(OperationContext* const opCtx,\n                            const CollectionPtr& coll,\n                            const NamespaceString& collectionName,\n                            const CollectionOptions& options,\n                            const BSONObj& idIndex,\n                            const OplogSlot& createOpTime,\n                            bool fromMigrate) override {\n        ReservedTimes times{opCtx};\n        for (auto& o : _observers)\n            o->onCreateCollection(\n                opCtx, coll, collectionName, options, idIndex, createOpTime, fromMigrate);\n    }\n\n using OpObserver::onInserts;\n    void onInserts(OperationContext* const opCtx,\n                   const NamespaceString& nss,\n                   const UUID& uuid,\n                   std::vector<InsertStatement>::const_iterator begin,\n                   std::vector<InsertStatement>::const_iterator end,\n                   bool fromMigrate) override {\n        ReservedTimes times{opCtx};\n        for (auto& o : _observers)\n            o->onInserts(opCtx, nss, uuid, begin, end, fromMigrate);\n    }\n")),(0,a.yg)("h2",{id:"understanding-write-concern"},"Understanding Write Concern"),(0,a.yg)("p",null,"Write concern is a way to ensure that the write operations are propagated to the secondary nodes."),(0,a.yg)("h3",{id:"default-write-concern"},"Default Write Concern"),(0,a.yg)("p",null,"If a write operation does not explicitly specify a write concern, the server will use a default\nwrite concern. "),(0,a.yg)("p",null,"This default write concern will be defined by either the"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Cluster-Wide write concern"),", explicitly set by the user\nor"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Implicit Default write concern"),", implicitly set by the server based on replica set configuration.")),(0,a.yg)("h4",{id:"cluster-wide-write-concern"},"Cluster-Wide Write Concern"),(0,a.yg)("p",null,"The cluster-wide write concern is set by the user using the ",(0,a.yg)("a",{parentName:"p",href:"https://docs.mongodb.com/manual/reference/command/setDefaultRWConcern/"},(0,a.yg)("inlineCode",{parentName:"a"},"setDefaultRWConcern"))," command. Setting the cluster-wide write concern will cause the implicit default write concern not to take effect."),(0,a.yg)("p",null,"On a sharded cluster, the cluster-wide write concern is set on the config server. On a replica set, the cluster-wide write concern is set on the primary node. The below code snippets shows how the cluster-wide write concern is set on the primary node and stored on the config node. "),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-javascript"},"db.adminCommand(\n  {\n    setDefaultRWConcern : 1,\n    defaultReadConcern: { <read concern> },\n    defaultWriteConcern: { <write concern> },\n    writeConcern: { <write concern> },\n    comment: <any>\n  }\n)\n")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-cpp"},'//cluster_rwc_defaults_commands.cpp \nclass ClusterSetDefaultRWConcernCommand : public BasicCommand {\npublic:\n    ClusterSetDefaultRWConcernCommand() : BasicCommand("setDefaultRWConcern") {}\n\n    bool run(OperationContext* opCtx,\n             const DatabaseName&,\n             const BSONObj& cmdObj,\n             BSONObjBuilder& result) override {\n        auto configShard = Grid::get(opCtx)->shardRegistry()->getConfigShard();\n        auto cmdResponse = uassertStatusOK(configShard->runCommandWithFixedRetryAttempts(\n            opCtx,\n            ReadPreferenceSetting(ReadPreference::PrimaryOnly),\n            NamespaceString::kAdminDb.toString(),\n            CommandHelpers::appendMajorityWriteConcern(\n                CommandHelpers::filterCommandRequestForPassthrough(cmdObj),\n                opCtx->getWriteConcern()),\n            Shard::RetryPolicy::kNotIdempotent));\n\n        uassertStatusOK(cmdResponse.commandStatus);\n        uassertStatusOK(cmdResponse.writeConcernStatus);\n\n        // Quickly pick up the new defaults by setting them in the cache.\n        auto newDefaults = RWConcernDefault::parse(IDLParserContext("ClusterSetDefaultRWConcern"),\n                                                   cmdResponse.response);\n        if (auto optWC = newDefaults.getDefaultWriteConcern()) {\n            if (optWC->hasCustomWriteMode()) {\n                LOGV2_WARNING(\n                    6081700,\n                    "A custom write concern is being set as the default write concern in a sharded "\n                    "cluster. This set is unchecked, but if the custom write concern does not "\n                    "exist on all shards in the cluster, errors will occur upon writes",\n                    "customWriteConcern"_attr = stdx::get<std::string>(optWC->w));\n            }\n        }\n        ReadWriteConcernDefaults::get(opCtx).setDefault(opCtx, std::move(newDefaults));\n\n        CommandHelpers::filterCommandReplyForPassthrough(cmdResponse.response, &result);\n        return true;\n    }\n')),(0,a.yg)("h4",{id:"implicit-default-write-concern"},"Implicit default write concern"),(0,a.yg)("p",null,"The implicit default write concern is calculated and set on startup by the server based on the replica set configuration. The server will set the implicit default write concern to the following:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"If the replica set has a single node, the implicit default write concern is ",(0,a.yg)("inlineCode",{parentName:"li"},"{ w: 1 }")),(0,a.yg)("li",{parentName:"ul"},"For most of the cases the implicit default write concern is ",(0,a.yg)("inlineCode",{parentName:"li"},'{ w: "majority" }'))),(0,a.yg)("h5",{id:"psa"},"PSA"),(0,a.yg)("p",null,(0,a.yg)("inlineCode",{parentName:"p"},"implicitDefaultWriteConcern = if ((#arbiters > 0) AND (#non-arbiters <= majority(#voting nodes)) then {w:1} else {w:majority}")),(0,a.yg)("p",null,"Implicit default to a value that the set can satisfy in the event of one data-bearing node\ngoing down. That is, the number of data-bearing nodes must be strictly greater than the majority\nof voting nodes for the set to set ",(0,a.yg)("inlineCode",{parentName:"p"},'{w: "majority"}'),"."),(0,a.yg)("p",null,"For example, if we have a PSA replica set, and the secondary goes down, the primary cannot\nsuccessfully acknowledge a majority write as the majority for the set is two nodes. However, the\nprimary will remain primary with the arbiter's vote. In this case, the DWCF will have preemptively\nset the IDWC to ",(0,a.yg)("inlineCode",{parentName:"p"},"{w: 1}")," so the user can still perform writes to the replica set."),(0,a.yg)("h5",{id:"sharded-cluster"},"Sharded Cluster"),(0,a.yg)("p",null,"For a sharded cluster, the implicit default write concern is set to ",(0,a.yg)("inlineCode",{parentName:"p"},'{ w: "majority" }')," if the\ncluster has a majority of voting nodes. Otherwise, the implicit default write concern is set to\n",(0,a.yg)("inlineCode",{parentName:"p"},"{ w: 1 }"),"."),(0,a.yg)("h2",{id:"understanding-secondary-nodes-operations"},"Understanding Secondary Nodes Operations"),(0,a.yg)("p",null,"The secondary nodes will choose the node with the highest ",(0,a.yg)("inlineCode",{parentName:"p"},"lastApplied")," timestamp as the",(0,a.yg)("strong",{parentName:"p"}," sync source"),". The secondary nodes will then ",(0,a.yg)("strong",{parentName:"p"},"pull")," the oplog entries from the sync source and apply them to its own oplog."),(0,a.yg)("p",null,"The Secondary will also keep its ",(0,a.yg)("strong",{parentName:"p"},"sync source")," uptodate with its progress, this helps primary satisfy the read concern. "),(0,a.yg)("p",null,"Here are the high level steps performed to select and probe the sync source"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("inlineCode",{parentName:"li"},"TopologyCoordinator")," checks if user requested a specific sync source using ",(0,a.yg)("inlineCode",{parentName:"li"},"replSetSyncFrom")," command. If so, it will use that sync source. Otherwise, it will use the sync source from the last successful election."),(0,a.yg)("li",{parentName:"ol"},"Check if ",(0,a.yg)("strong",{parentName:"li"},"chaining")," is disabled. If so, the secondary will always use primary as its sync source ")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-cpp"},' if (chainingPreference == ChainingPreference::kUseConfiguration &&\n        !_rsConfig.isChainingAllowed()) {\n        if (_currentPrimaryIndex == -1) {\n            LOG(1) << "Cannot select a sync source because chaining is"\n                      " not allowed and primary is unknown/down";\n            _syncSource = HostAndPort();\n            return _syncSource;\n        } else if (_memberIsBlacklisted(*_currentPrimaryMember(), now)) {\n            LOG(1) << "Cannot select a sync source because chaining is not allowed and primary "\n                      "member is blacklisted: "\n                   << _currentPrimaryMember()->getHostAndPort();\n            _syncSource = HostAndPort();\n            return _syncSource;\n\n')),(0,a.yg)("ol",{start:3},(0,a.yg)("li",{parentName:"ol"},"Fetch latest opTime. Do not sync from a node where newest oplog is more than ",(0,a.yg)("inlineCode",{parentName:"li"},"maxSyncSourceLagSecs"))),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-cpp"},"    if (_currentPrimaryIndex != -1) {\n        OpTime primaryOpTime = _memberData.at(_currentPrimaryIndex).getHeartbeatAppliedOpTime();\n\n        // Check if primaryOpTime is still close to 0 because we haven't received\n        // our first heartbeat from a new primary yet.\n        unsigned int maxLag =\n            static_cast<unsigned int>(durationCount<Seconds>(_options.maxSyncSourceLagSecs));\n        if (primaryOpTime.getSecs() >= maxLag) {\n            oldestSyncOpTime =\n                OpTime(Timestamp(primaryOpTime.getSecs() - maxLag, 0), primaryOpTime.getTerm());\n        }\n    }\n")),(0,a.yg)("ol",{start:4},(0,a.yg)("li",{parentName:"ol"},"Loop through all the nodes and find the closest node which satisfies the condition ")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-cpp"},"HostAndPort TopologyCoordinator::chooseNewSyncSource(Date_t now,\n                                                     const OpTime& lastOpTimeFetched,\n                                                     ChainingPreference chainingPreference) {\n\n...\n...\n...\n")),(0,a.yg)("h3",{id:"oplog-fetching"},"Oplog Fetching"),(0,a.yg)("p",null,"The secondary node will fetch the oplog entries from the sync source to keep its data syncronized. The entire implementation of the oplog fetching is in the ",(0,a.yg)("inlineCode",{parentName:"p"},"OplogFetcher")," class which runs in a separate thread and communicates via a dedicated client connection."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-cpp"},"\nvoid OplogFetcher::setConnection(std::unique_ptr<DBClientConnection>&& _connectedClient) {\n    // Can only call this once, before startup.\n    invariant(!_conn);\n    _conn = std::move(_connectedClient);\n}\n\n")))}m.isMDXComponent=!0}}]);